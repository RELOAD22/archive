{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sentence_list_fname = './assignment_training_data_word_segment.json'\n",
    "sentence_list = json.load(open(sentence_list_fname , 'r',encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "word = []\n",
    "for index in range(3000):\n",
    "    a = sentence_list[index]['indexes']\n",
    "    b = [str(i) for i in a]\n",
    "    word.append(b)\n",
    "#word = ' '.join(word)\n",
    "print(word)\n",
    "\n",
    "L = 10\n",
    "print('training...')\n",
    "model = Word2Vec(word, size=L, window=5, min_count=5, workers=multiprocessing.cpu_count(),\n",
    "                 iter=10)\n",
    "# model.save('Word2VecModel.m')\n",
    "#model.wv.save_word2vec_format('Word2VecModel.vector', binary=False)\n",
    "print('outputing...')\n",
    "\n",
    "print(model)\n",
    "print(model['0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentence_list[0]\n",
    "print (sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for index in range(3000):\n",
    "    temp_len = len(sentence_list[index]['words']) \n",
    "    if temp_len > max_length:\n",
    "        max_length = temp_len\n",
    "print(max_length)\n",
    "\n",
    "all_result = 0\n",
    "for index in range(3000):\n",
    "    temp_len = len(sentence_list[index]['results']) \n",
    "    all_result += temp_len\n",
    "print(\"1-result:\",all_result)\n",
    "\n",
    "all_result_0 = 0\n",
    "for index in range(3000):\n",
    "    temp_len = len(sentence_list[index]['times'])*len(sentence_list[index]['values'])*len(sentence_list[index]['attributes'])\n",
    "    all_result_0 += temp_len\n",
    "print(\"0-result:\",all_result_0 - all_result)\n",
    "\n",
    "print(\"result:\", all_result_0)\n",
    "\n",
    "import random\n",
    "true_num = 0\n",
    "not_true_num = 0\n",
    "data = []\n",
    "data_y = []\n",
    "print(len(data))\n",
    "total_index = 0\n",
    "for index in range(3000):\n",
    "    for time in sentence_list[index]['times']:\n",
    "        for attribute in sentence_list[index]['attributes']:\n",
    "            for value in sentence_list[index]['values']:\n",
    "                condition = [time,attribute,value]\n",
    "                if condition in sentence_list[index]['results']:\n",
    "                    true_flag = 1\n",
    "                    true_num += 1\n",
    "                    data_y.append([1,0])\n",
    "                else:\n",
    "                    true_flag = 0\n",
    "                    r_del = random.randint(0, 18)\n",
    "                    if r_del != 0:\n",
    "                        continue\n",
    "                    not_true_num += 1\n",
    "                    data_y.append([0,1])\n",
    "                word_temp = []\n",
    "                sen_vec = []\n",
    "                word_index = 0\n",
    "                for word in sentence_list[index]['indexes']:\n",
    "                    if (true_flag == 1) and (word_index in condition):\n",
    "                        add_item = 10\n",
    "                    else:\n",
    "                        add_item = 0\n",
    "                    if str(word) in model:\n",
    "                        word_vec = list(model[str(word)])\n",
    "                        word_vec.append(add_item)\n",
    "                        word_vec.append(add_item)\n",
    "                        word_vec.append(add_item)\n",
    "                        word_vec.append(add_item)\n",
    "                    else:\n",
    "                        word_vec = [0]*14\n",
    "                    sen_vec += word_vec \n",
    "                    word_index += 1\n",
    "                while len(sen_vec) < 2156:\n",
    "                    sen_vec += [0]*14\n",
    "                data.append(sen_vec)\n",
    "                \n",
    "\n",
    "print(len(data))\n",
    "print(len(data_y))\n",
    "print(true_num)\n",
    "print(not_true_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_x = np.array(data)\n",
    "raw_y = np.array(data_y)\n",
    "print(raw_x.shape)\n",
    "print(raw_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分出训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(raw_x, raw_y, test_size=0.30, random_state=6)\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  tensorflow as tf\n",
    "from tensorflow.contrib import  rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_batch(x,y, batch_size):\n",
    "    total_len = len(x)\n",
    "    begin_index = random.randint(0, total_len - batch_size)\n",
    "    end_index = begin_index + batch_size\n",
    "    select_x = x[begin_index:end_index,:]\n",
    "    select_y = y[begin_index:end_index,:]\n",
    "    return (select_x, select_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainimgs, trainlabels, testimgs, testlabels = x_train, y_train, x_validation, y_validation\n",
    "ntrain, ntest, dim, nclasses =trainimgs.shape[0], testimgs.shape[0],trainimgs.shape[1],trainlabels.shape[1]\n",
    "print(ntrain, ntest, dim, nclasses)\n",
    "print (\"MNIST loaded\")\n",
    "print(trainimgs.shape)\n",
    "print(trainlabels.shape)\n",
    "\n",
    "\n",
    "x = get_batch(trainimgs,trainlabels, 1280)\n",
    "print(x[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rate=0.01\n",
    "train_step=100\n",
    "batch_size=1280\n",
    "display_step=1\n",
    "\n",
    "frame_size=14\n",
    "sequence_length=154\n",
    "hidden_num=200\n",
    "n_classes=2\n",
    "\n",
    "#定义输入,输出\n",
    "x=tf.placeholder(dtype=tf.float32,shape=[None,sequence_length*frame_size],name=\"inputx\")\n",
    "y=tf.placeholder(dtype=tf.float32,shape=[None,n_classes],name=\"expected_y\")\n",
    "#定义权值\n",
    "weights=tf.Variable(tf.truncated_normal(shape=[hidden_num,n_classes]))\n",
    "bias=tf.Variable(tf.zeros(shape=[n_classes]))\n",
    "\n",
    "def RNN(x,weights,bias):\n",
    "    x=tf.reshape(x,shape=[-1,sequence_length,frame_size])\n",
    "    rnn_cell=tf.nn.rnn_cell.BasicRNNCell(hidden_num)\n",
    "    init_state=tf.zeros(shape=[batch_size,rnn_cell.state_size])\n",
    "    output,states=tf.nn.dynamic_rnn(rnn_cell,x,dtype=tf.float32)\n",
    "    return tf.nn.softmax(tf.matmul(output[:,-1,:],weights)+bias,1)\n",
    "predy=RNN(x,weights,bias)\n",
    "cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predy,labels=y))\n",
    "train=tf.train.AdamOptimizer(train_rate).minimize(cost)\n",
    "\n",
    "correct_pred=tf.equal(tf.argmax(predy,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.to_float(correct_pred))\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "step=1\n",
    "testx,testy=get_batch(testimgs,testlabels, batch_size)\n",
    "while step<train_step:\n",
    "    batch_x,batch_y=get_batch(trainimgs,trainlabels, batch_size)\n",
    "#    batch_x=tf.reshape(batch_x,shape=[batch_size,sequence_length,frame_size])\n",
    "    _loss,__=sess.run([cost,train],feed_dict={x:batch_x,y:batch_y})\n",
    "    if step % display_step ==0:\n",
    "\n",
    "        acc,loss=sess.run([accuracy,cost],feed_dict={x:testx,y:testy})\n",
    "        print(step,acc,loss)\n",
    "\n",
    "    step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc,loss=sess.run([accuracy,cost],feed_dict={x:x_validation,y:y_validation})\n",
    "print(acc,loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
